{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ljf1/dis/lora/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "from accelerate import Accelerator\n",
    "\n",
    "from preprocessor import load_and_preprocess, decoding, process_data\n",
    "from qwen import load_qwen\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.optim.lr_scheduler import StepLR, CosineAnnealingLR\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "from preprocessor import get_dataset\n",
    "\n",
    "import wandb\n",
    "import joblib\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for matplotlib plots\n",
    "SMALL_SIZE = 15+5\n",
    "MEDIUM_SIZE = 20+5\n",
    "BIGGER_SIZE = 25+5\n",
    "\n",
    "plt.rc('font', size=SMALL_SIZE)\n",
    "plt.rc('axes', titlesize=SMALL_SIZE)\n",
    "plt.rc('axes', labelsize=MEDIUM_SIZE)\n",
    "plt.rc('xtick', labelsize=SMALL_SIZE)\n",
    "plt.rc('ytick', labelsize=SMALL_SIZE)\n",
    "plt.rc('legend', fontsize=SMALL_SIZE)\n",
    "plt.rc('figure', titlesize=BIGGER_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_overall_params = joblib.load(\"best_overall_params.joblib\")\n",
    "\n",
    "lora_rank = best_overall_params['lora_rank']\n",
    "lora_alpha = 2*lora_rank\n",
    "batch_size = 4\n",
    "learning_rate = best_overall_params['learning_rate']\n",
    "test_size = 0.2\n",
    "max_steps = 200\n",
    "max_ctx_length = best_overall_params['max_ctx_length']\n",
    "points = 80\n",
    "\n",
    "schedulers = ['StepLR', 'CosineAnnealingLR', 'CosineScheduleWithWarmup']\n",
    "\n",
    "schedule_choice = schedulers[2]\n",
    "\n",
    "if schedule_choice == 'StepLR':\n",
    "    step_size = 100\n",
    "    gamma = 0.1\n",
    "elif schedule_choice == 'CosineAnnealingLR':\n",
    "    T_max = max_steps\n",
    "elif schedule_choice == 'CosineScheduleWithWarmup':\n",
    "    warmup_steps = int(0.075*max_steps)\n",
    "    num_training_steps = max_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRALinear(nn.Module):\n",
    "    def __init__(self, original_linear: nn.Linear, r: int, alpha: int = None):\n",
    "        super().__init__()\n",
    "        assert isinstance(original_linear, nn.Linear)\n",
    "        self.original_linear = original_linear\n",
    "        self.original_linear.weight.requires_grad = False\n",
    "        if self.original_linear.bias is not None:\n",
    "            self.original_linear.bias.requires_grad = False\n",
    "        in_dim = original_linear.in_features\n",
    "        out_dim = original_linear.out_features\n",
    "        self.r = r\n",
    "        self.alpha = alpha if alpha else r\n",
    "\n",
    "        device = original_linear.weight.device\n",
    "        self.A = nn.Parameter(torch.empty(r, in_dim, device=device))\n",
    "        self.B = nn.Parameter(torch.zeros(out_dim, r, device=device))\n",
    "        \n",
    "        # Initialise A with He initialization\n",
    "        nn.init.kaiming_normal_(self.A, nonlinearity=\"linear\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        base_out = self.original_linear(x)\n",
    "        lora_out = (x @ self.A.T) @ self.B.T\n",
    "        return base_out + lora_out * (self.alpha / self.r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = load_qwen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the data into sequences of text\n",
    "train_texts, val_texts, test_texts = load_and_preprocess(\"lotka_volterra_data.h5\", test_size=test_size)\n",
    "\n",
    "# ^Each of these is a `list[str]` representing contiguous parts of the time series,\n",
    "#  in text form (using the LLMTIME scheme).\n",
    "\n",
    "# Modified tokenization with chunking\n",
    "def process_sequences(texts, tokenizer, max_length=512, stride=256):\n",
    "    all_input_ids = []\n",
    "    for text in texts:\n",
    "        # Apply Qwen's tokenization scheme to the text:\n",
    "        encoding = tokenizer(text, return_tensors=\"pt\", add_special_tokens=False, padding_side='left')\n",
    "        seq_ids = encoding.input_ids[0]\n",
    "\n",
    "        # Create sliding windows to further divide the data into chunks:\n",
    "        for i in range(0, len(seq_ids), stride):\n",
    "            chunk = seq_ids[i : i + max_length]\n",
    "            if len(chunk) < max_length:\n",
    "                chunk = torch.cat(\n",
    "                    [\n",
    "                        torch.full((max_length - len(chunk),), tokenizer.pad_token_id),\n",
    "                        chunk,\n",
    "                    ]\n",
    "                )\n",
    "            all_input_ids.append(chunk)\n",
    "    return torch.stack(all_input_ids)\n",
    "\n",
    "\n",
    "def process_data(texts, tokenizer, points=80):\n",
    "    given_input_ids = []\n",
    "    for text in texts:\n",
    "        given_text = ';'.join([chunk for i, chunk in enumerate(text.split(';')) if i < points])\n",
    "        encoding_given = tokenizer(given_text, return_tensors=\"pt\", padding='max_length', padding_side='left', max_length=1200)\n",
    "        given_input_ids.append(encoding_given.input_ids[0])\n",
    "    return np.stack([text for text in texts]), torch.stack(given_input_ids)\n",
    "\n",
    "def running_mse(prediction, actual):\n",
    "    mse = []\n",
    "    for i in range(len(prediction)):\n",
    "        mse.append(mean_squared_error(prediction[:i+1], actual[:i+1]))\n",
    "    return mse\n",
    "\n",
    "def evaluate_model(model, val_loader, step, max_batches=None):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (batch,) in enumerate(tqdm(val_loader, desc=\"val set\")):\n",
    "            # Exit loop after processing max_batches\n",
    "            if max_batches is not None and batch_idx >= max_batches:\n",
    "                break\n",
    "            outputs = model(batch, labels=batch)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "            del outputs\n",
    "            del loss\n",
    "    \n",
    "    # Calculate metrics - divide by actual number of batches processed\n",
    "    num_batches = min(len(val_loader), max_batches) if max_batches is not None else len(val_loader)\n",
    "    avg_loss = total_loss / num_batches\n",
    "\n",
    "    print(f'Loss on validation subset ({num_batches}/{len(val_loader)} batches) at step {step}: {avg_loss:.4f}')\n",
    "    return avg_loss\n",
    "\n",
    "# Defines the maximum context length for the model\n",
    "train_input_ids = process_sequences(\n",
    "    train_texts, tokenizer, max_ctx_length, stride=max_ctx_length // 2\n",
    ")\n",
    "val_input_ids = process_sequences(\n",
    "    val_texts, tokenizer, max_ctx_length, stride=max_ctx_length\n",
    ")\n",
    "test_texts_all, test_input_ids_some = process_data(\n",
    "    test_texts, tokenizer, points=points\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_dataset = TensorDataset(train_input_ids)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "\n",
    "val_dataset = TensorDataset(val_input_ids)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "\n",
    "test_dataset = TensorDataset(test_input_ids_some)\n",
    "test_loader = DataLoader(test_dataset, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Training with max_ctx_length=768\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Dictionary to store results\n",
    "grid_results = {}\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"Training with max_ctx_length={max_ctx_length}\")\n",
    "print(f\"{'='*50}\\n\")\n",
    "\n",
    "# Apply LoRA with current rank\n",
    "for layer in model.model.layers:\n",
    "    layer.self_attn.q_proj = LoRALinear(layer.self_attn.q_proj, r=lora_rank, alpha=lora_alpha)\n",
    "    layer.self_attn.v_proj = LoRALinear(layer.self_attn.v_proj, r=lora_rank, alpha=2*lora_alpha)\n",
    "\n",
    "# Create optimizer with current learning rate\n",
    "optimizer = torch.optim.Adam(\n",
    "    (p for p in model.parameters() if p.requires_grad), \n",
    "    lr=learning_rate,\n",
    ")\n",
    "\n",
    "# Create scheduler\n",
    "if schedule_choice == 'StepLR':\n",
    "    scheduler = StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "elif schedule_choice == 'CosineAnnealingLR':\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=T_max)\n",
    "elif schedule_choice == 'CosineScheduleWithWarmup':\n",
    "    scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=num_training_steps)\n",
    "\n",
    "#Prepare with accelerator\n",
    "accelerator = Accelerator(mixed_precision='fp16')\n",
    "model, optimizer, scheduler, train_loader_local, val_loader_local = accelerator.prepare(\n",
    "    model, optimizer, scheduler, train_loader, val_loader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val set: 100%|██████████| 50/50 [02:03<00:00,  2.47s/it]\n",
      "Steps 0:   0%|          | 1/800 [02:26<32:27:00, 146.21s/it, loss=3.93]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on validation subset (50/50 batches) at step 0: 3.8824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val set: 100%|██████████| 50/50 [01:37<00:00,  1.94s/it], loss=3.84]   \n",
      "Steps 0:   6%|▋         | 51/800 [07:52<6:59:53, 33.64s/it, loss=2.43]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on validation subset (50/50 batches) at step 50: 3.8824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val set: 100%|██████████| 50/50 [03:03<00:00,  3.66s/it]t, loss=2.38] \n",
      "Steps 0:  13%|█▎        | 101/800 [14:54<11:33:28, 59.53s/it, loss=6.71]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on validation subset (50/50 batches) at step 100: 3.8824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val set: 100%|██████████| 50/50 [01:41<00:00,  2.04s/it]/it, loss=5.32] \n",
      "Steps 0:  19%|█▉        | 151/800 [27:16<7:50:46, 43.52s/it, loss=1.1] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on validation subset (50/50 batches) at step 150: 3.8824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps 0:  25%|██▍       | 199/800 [33:59<1:42:39, 10.25s/it, loss=5.19]\n",
      "val set: 100%|██████████| 50/50 [01:14<00:00,  1.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on validation subset (50/50 batches) at step 200: 3.8824\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train the model (shortened training for grid search)\n",
    "steps = 0\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "early_stop_steps = min(max_steps, 500)  # Reduce training for grid search\n",
    "\n",
    "while steps < early_stop_steps:\n",
    "    progress_bar = tqdm(train_loader_local, desc=f\"Steps {steps}\")\n",
    "    for (batch,) in progress_bar:\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch, labels=batch)\n",
    "        loss = outputs.loss\n",
    "        train_losses.append([loss.item(), steps])\n",
    "        accelerator.backward(loss)\n",
    "        optimizer.step()\n",
    "        \n",
    "        if steps % 50 == 0:\n",
    "            avg_loss = evaluate_model(model, val_loader_local, steps)\n",
    "            val_losses.append([avg_loss, steps])\n",
    "            model.train()\n",
    "            \n",
    "        steps += 1\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "        del loss\n",
    "        del outputs\n",
    "        del batch\n",
    "        \n",
    "        if steps >= early_stop_steps:\n",
    "            break\n",
    "\n",
    "# Final evaluation\n",
    "final_val_loss = evaluate_model(model, val_loader_local, steps)\n",
    "\n",
    "# Store results\n",
    "grid_results[(schedule_choice)] = {\n",
    "    \"final_val_loss\": final_val_loss,\n",
    "    \"train_losses\": train_losses,\n",
    "    \"val_losses\": val_losses,\n",
    "}\n",
    "\n",
    "if schedule_choice == 'StepLR':\n",
    "    grid_results[(schedule_choice)][\"step_size\"] = step_size\n",
    "    grid_results[(schedule_choice)][\"gamma\"] = gamma\n",
    "elif schedule_choice == 'CosineAnnealingLR':\n",
    "    grid_results[(schedule_choice)][\"T_max\"] = T_max\n",
    "elif schedule_choice == 'CosineScheduleWithWarmup':\n",
    "    grid_results[(schedule_choice)][\"warmup_steps\"] = warmup_steps\n",
    "    grid_results[(schedule_choice)][\"num_training_steps\"] = num_training_steps\n",
    "\n",
    "\n",
    "del model\n",
    "del tokenizer\n",
    "del optimizer\n",
    "del train_loader_local\n",
    "del val_loader_local\n",
    "del accelerator\n",
    "del train_losses\n",
    "del val_losses\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../results/grid_results_CosineScheduleWithWarmup.joblib']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(grid_results, f\"../results/grid_results_{schedule_choice}.joblib\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lora",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
