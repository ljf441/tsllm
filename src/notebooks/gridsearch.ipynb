{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "from accelerate import Accelerator\n",
    "\n",
    "from preprocessor import load_and_preprocess, decoding, process_data\n",
    "from qwen import load_qwen\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.optim.lr_scheduler import StepLR, CosineAnnealingLR\n",
    "from preprocessor import get_dataset\n",
    "\n",
    "import wandb\n",
    "import joblib\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torchtune\n",
    "\n",
    "import gc\n",
    "\n",
    "from flopper import Flopper\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for matplotlib plots\n",
    "SMALL_SIZE = 15+5\n",
    "MEDIUM_SIZE = 20+5\n",
    "BIGGER_SIZE = 25+5\n",
    "\n",
    "plt.rc('font', size=SMALL_SIZE)\n",
    "plt.rc('axes', titlesize=SMALL_SIZE)\n",
    "plt.rc('axes', labelsize=MEDIUM_SIZE)\n",
    "plt.rc('xtick', labelsize=SMALL_SIZE)\n",
    "plt.rc('ytick', labelsize=SMALL_SIZE)\n",
    "plt.rc('legend', fontsize=SMALL_SIZE)\n",
    "plt.rc('figure', titlesize=BIGGER_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define LoRA layers\n",
    "class LoRALinear(nn.Module):\n",
    "    def __init__(self, original_linear: nn.Linear, r: int, alpha: int = None):\n",
    "        super().__init__()\n",
    "        assert isinstance(original_linear, nn.Linear)\n",
    "        self.original_linear = original_linear\n",
    "        self.original_linear.weight.requires_grad = False\n",
    "        if self.original_linear.bias is not None:\n",
    "            self.original_linear.bias.requires_grad = False\n",
    "        in_dim = original_linear.in_features\n",
    "        out_dim = original_linear.out_features\n",
    "        self.r = r\n",
    "        self.alpha = alpha if alpha else r\n",
    "\n",
    "        device = original_linear.weight.device\n",
    "        self.A = nn.Parameter(torch.empty(r, in_dim, device=device))\n",
    "        self.B = nn.Parameter(torch.zeros(out_dim, r, device=device))\n",
    "        \n",
    "        # Initialise A with He initialization\n",
    "        nn.init.kaiming_normal_(self.A, nonlinearity=\"linear\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        base_out = self.original_linear(x)\n",
    "        lora_out = (x @ self.A.T) @ self.B.T\n",
    "        return base_out + lora_out * (self.alpha / self.r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model and tokenizer\n",
    "model, tokenizer = load_qwen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified tokenization with chunking\n",
    "def process_sequences(texts, tokenizer, max_length=512, stride=256):\n",
    "    all_input_ids = []\n",
    "    for text in texts:\n",
    "        # Apply Qwen's tokenization scheme to the text:\n",
    "        encoding = tokenizer(text, return_tensors=\"pt\", add_special_tokens=False, padding_side='left')\n",
    "        seq_ids = encoding.input_ids[0]\n",
    "\n",
    "        # Create sliding windows to further divide the data into chunks:\n",
    "        for i in range(0, len(seq_ids), stride):\n",
    "            chunk = seq_ids[i : i + max_length]\n",
    "            if len(chunk) < max_length:\n",
    "                chunk = torch.cat(\n",
    "                    [\n",
    "                        torch.full((max_length - len(chunk),), tokenizer.pad_token_id),\n",
    "                        chunk,\n",
    "                    ]\n",
    "                )\n",
    "            all_input_ids.append(chunk)\n",
    "    return torch.stack(all_input_ids)\n",
    "\n",
    "# Process the testing data into sequences of text as well as input IDs\n",
    "def process_data(texts, tokenizer, points=80):\n",
    "    \"\"\"\n",
    "    Process the data into sequences of text\n",
    "    \n",
    "    Args:\n",
    "        texts: list of original strings\n",
    "        tokenizer: tokenizer object\n",
    "        points: number of points to give to the model\n",
    "        \n",
    "    Returns:\n",
    "        np.array: texts\n",
    "        torch.Tensor: given_input_ids\n",
    "    \"\"\"\n",
    "    given_input_ids = []\n",
    "    for text in texts:\n",
    "        given_text = ';'.join([chunk for i, chunk in enumerate(text.split(';')) if i < points])\n",
    "        encoding_given = tokenizer(given_text, return_tensors=\"pt\", padding='max_length', padding_side='left', max_length=1200)\n",
    "        given_input_ids.append(encoding_given.input_ids[0])\n",
    "    return np.stack([text for text in texts]), torch.stack(given_input_ids)\n",
    "\n",
    "def running_mse(prediction, actual):\n",
    "    \"\"\"\n",
    "    Calculate the running mean squared error.\n",
    "\n",
    "    Args:\n",
    "        prediction: list of predicted values\n",
    "        actual: list of actual values\n",
    "\n",
    "    Returns:\n",
    "        np.array: running mean squared error\n",
    "    \"\"\"\n",
    "    mse = []\n",
    "    for i in range(len(prediction)):\n",
    "        mse.append(mean_squared_error(prediction[:i+1], actual[:i+1]))\n",
    "    return np.array(mse)\n",
    "\n",
    "def evaluate_model(model, val_loader, step):\n",
    "    \"\"\"\n",
    "    Evaluate the model on the validation set.\n",
    "    \n",
    "    Args:\n",
    "        model: model to evaluate\n",
    "        val_loader: validation data loader\n",
    "        step: current step\n",
    "\n",
    "    Returns:\n",
    "        float: average loss on the validation set\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (batch,) in enumerate(tqdm(val_loader, desc=\"val set\")):\n",
    "            outputs = model(batch, labels=batch)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "    # Calculate metrics\n",
    "    num_batches = len(val_loader)\n",
    "    avg_loss = total_loss / len(val_loader)\n",
    "    print(f'Loss on validation subset ({num_batches}/{len(val_loader)} batches) at step {step}: {avg_loss:.4f}')\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants\n",
    "batch_size = 4\n",
    "test_size = 0.2\n",
    "max_steps = 200\n",
    "max_ctx_length = 512\n",
    "points = 80\n",
    "\n",
    "# Define parameter grid\n",
    "lora_ranks = [2, 4, 8]\n",
    "learning_rates = [1e-5, 5e-5, 1e-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for rank in lora_ranks:\n",
    "    for lr in learning_rates:\n",
    "        lora_rank = rank\n",
    "        learning_rate = lr\n",
    "        lora_alpha = 2*lora_rank\n",
    "\n",
    "        # Load the model and tokenizer\n",
    "        model, tokenizer = load_qwen()\n",
    "\n",
    "        train_texts, val_texts, test_texts = load_and_preprocess(\"lotka_volterra_data.h5\", test_size=test_size)\n",
    "\n",
    "        # Defines the maximum context length for the model\n",
    "        train_input_ids = process_sequences(\n",
    "            train_texts, tokenizer, max_ctx_length, stride=max_ctx_length // 2\n",
    "        )\n",
    "        val_input_ids = process_sequences(\n",
    "            val_texts, tokenizer, max_ctx_length, stride=max_ctx_length\n",
    "        )\n",
    "        test_texts_all, test_input_ids_some = process_data(\n",
    "            test_texts, tokenizer, points=points\n",
    "        )\n",
    "\n",
    "        # Create data loaders\n",
    "        train_dataset = TensorDataset(train_input_ids)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        val_dataset = TensorDataset(val_input_ids)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        test_dataset = TensorDataset(test_input_ids_some)\n",
    "        test_loader = DataLoader(test_dataset, shuffle=False)\n",
    "\n",
    "        # Dictionary to store results\n",
    "        grid_results = {}\n",
    "\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Training with lora_rank={lora_rank}, learning_rate={learning_rate}\")\n",
    "        print(f\"{'='*50}\\n\")\n",
    "\n",
    "        # Actually apply LoRA to the model:\n",
    "        for layer in model.model.layers:\n",
    "            layer.self_attn.q_proj = LoRALinear(layer.self_attn.q_proj, r=lora_rank, alpha = lora_alpha)\n",
    "            layer.self_attn.v_proj = LoRALinear(layer.self_attn.v_proj, r=lora_rank, alpha = lora_alpha)\n",
    "        # ^These are the parts that will actually be trained!\n",
    "\n",
    "        # Freeze all layers except the LoRA layers\n",
    "        for name, param in model.named_parameters():\n",
    "            if \"A\" in name or \"B\" in name:\n",
    "                param.requires_grad = True\n",
    "            else:\n",
    "                param.requires_grad = False\n",
    "\n",
    "        # Create optimizer with current learning rate\n",
    "        optimizer = torch.optim.Adam(\n",
    "            (p for p in model.parameters() if p.requires_grad), \n",
    "            lr=learning_rate, \n",
    "        )\n",
    "\n",
    "        # Prepare with accelerator\n",
    "        # Use mixed precision training as it is faster and consumes less memory\n",
    "        accelerator = Accelerator()\n",
    "        model, optimizer, train_loader_local, val_loader_local, test_loader_local = accelerator.prepare(\n",
    "            model, optimizer, train_loader, val_loader, test_loader\n",
    "        )\n",
    "\n",
    "        # Train the model (shortened training for grid search)\n",
    "        steps = 0\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        early_stop_steps = min(max_steps, 500)  # Reduce training for grid search\n",
    "\n",
    "        while steps < early_stop_steps:\n",
    "            progress_bar = tqdm(train_loader_local, desc=f\"Steps {steps}\")\n",
    "            for (batch,) in progress_bar:\n",
    "                model.train()\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(batch, labels=batch)\n",
    "                loss = outputs.loss\n",
    "                train_losses.append([loss.item(), steps])\n",
    "                accelerator.backward(loss)\n",
    "                optimizer.step()\n",
    "                \n",
    "                if (steps % 50) == 0:\n",
    "                    avg_loss = evaluate_model(model, val_loader_local, steps)\n",
    "                    val_losses.append([avg_loss, steps])\n",
    "                    model.train()\n",
    "                    \n",
    "                steps += 1\n",
    "                progress_bar.set_postfix(loss=loss.item())\n",
    "                \n",
    "                if steps >= early_stop_steps:\n",
    "                    break\n",
    "\n",
    "        # Final evaluation\n",
    "        final_val_loss = evaluate_model(model, val_loader_local, steps)\n",
    "\n",
    "        # Calculate the FLOPs\n",
    "        model_flops = Flopper(sequence_length = max_ctx_length, model=model, num_steps_training=max_steps, batch_size = batch_size, use_lora=True, lora_rank=lora_rank)\n",
    "        flops = model_flops.compute_flops()\n",
    "        print(f\"FLOPs: {flops}\")\n",
    "\n",
    "        # Store results\n",
    "        grid_results[(lora_rank, learning_rate)] = {\n",
    "            \"final_val_loss\": final_val_loss,\n",
    "            \"train_losses\": train_losses,\n",
    "            \"val_losses\": val_losses,\n",
    "            \"flops\": model_flops\n",
    "        }\n",
    "\n",
    "        # Save results\n",
    "        joblib.dump(grid_results, f\"../results/grid_results_{lora_rank}_{learning_rate}.joblib\")\n",
    "\n",
    "        # Test the model\n",
    "        model.eval()\n",
    "        with torch.no_grad():    \n",
    "            for (batch,) in tqdm(test_loader_local):\n",
    "                outputs = model.generate(batch, attention_mask = torch.ones_like(batch), max_new_tokens=max_ctx_length*2)\n",
    "                prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "                prediction_decoded = decoding(prediction)\n",
    "                break\n",
    "\n",
    "        test_decoded = decoding(test_texts_all[0])\n",
    "\n",
    "        colours = ['#7b3294', '#c2a5cf', '#a6dba0', '#008837']\n",
    "        _, _, time = get_dataset()\n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "        ax.plot(time, prediction_decoded[0], label='Prey (predicted)', linestyle=\"--\", zorder=100, color=colours[0], linewidth=2)\n",
    "        ax.plot(time, prediction_decoded[1], label='Predator (predicted)', linestyle=\"--\", zorder=100, color=colours[3], linewidth=2)\n",
    "        ax.plot(time, test_decoded[0], label='Prey (true)', color=colours[1], linewidth=2)\n",
    "        ax.plot(time, test_decoded[1], label='Predator (true)', color=colours[2], linewidth=2)\n",
    "        ax.axvline(time[79], color='black', linestyle='--', label='Prediction start')\n",
    "        ax.set_xlabel('Time')\n",
    "        ax.set_ylabel('Population')\n",
    "        ax.legend(fontsize=15, ncol=5, bbox_to_anchor=(0.5, 1.1), loc='center')\n",
    "        plt.savefig(f'../plots/{lora_rank}_{learning_rate}_inspection.png')\n",
    "        plt.show()\n",
    "\n",
    "        # Clean up\n",
    "        del model\n",
    "        del tokenizer\n",
    "        del optimizer\n",
    "        del train_loader_local\n",
    "        del val_loader_local\n",
    "        del test_loader_local\n",
    "        del accelerator\n",
    "        del train_losses\n",
    "        del val_losses\n",
    "        torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_overall_params = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_loss = 1000\n",
    "best_params = None\n",
    "\n",
    "all_train_loss = []\n",
    "all_val_loss = []\n",
    "all_keys = []\n",
    "\n",
    "best_val_loss = 1000\n",
    "best_file = None\n",
    "\n",
    "# Find all grid_results joblib files in the results folder\n",
    "pattern = r'grid_results_\\d+_[\\d.e+-]+\\.joblib'\n",
    "\n",
    "\n",
    "# Get all files in the results directory\n",
    "result_files = glob.glob('../results/*')\n",
    "\n",
    "# Filter files that match the pattern\n",
    "result_files = [file for file in result_files if re.search(pattern, file)]\n",
    "print(result_files)\n",
    "\n",
    "# Display the content of each file\n",
    "for file_path in result_files:\n",
    "    # Load the joblib file\n",
    "    result = joblib.load(file_path)\n",
    "    \n",
    "    key = next(iter(result.keys()))\n",
    "    print(f\"Key: {key}: \", result[key]['final_val_loss'])\n",
    "\n",
    "    all_train_loss.append(result[key]['train_losses'])\n",
    "    all_val_loss.append(result[key]['val_losses'])\n",
    "    all_keys.append(key)\n",
    "    \n",
    "    # Check if the current file has the best validation loss\n",
    "    if result[key]['final_val_loss'] < best_val_loss:\n",
    "        best_val_loss = result[key]['final_val_loss']\n",
    "        best_file = key\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"Best validation loss: {best_val_loss}\")\n",
    "print(f\"Best file: {best_file}\")\n",
    "print(f\"{'='*50}\")\n",
    "\n",
    "best_overall_params['lora_rank'] = best_file[0]\n",
    "best_overall_params['learning_rate'] = best_file[1]\n",
    "\n",
    "all_train_loss = np.array(all_train_loss)\n",
    "all_val_loss = np.array(all_val_loss)\n",
    "all_keys = np.array(all_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "for i in range(len(all_train_loss)):\n",
    "    ax.plot(all_train_loss[i, :, 1], all_train_loss[i, :, 0], label=f\"{all_keys[i]}\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "for i in range(len(all_val_loss)):\n",
    "    ax.plot(all_val_loss[i, :, 1], all_val_loss[i, :, 0], label=f\"{all_keys[i]}\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_overall_params_load = joblib.load(\"best_overall_params.joblib\")\n",
    "\n",
    "batch_size = 4\n",
    "test_size = 0.2\n",
    "max_steps = 200\n",
    "points = 80\n",
    "lora_rank = best_overall_params_load['lora_rank']\n",
    "lora_alpha = 2*lora_rank\n",
    "learning_rate = best_overall_params_load['learning_rate']\n",
    "\n",
    "ctx_lengths = [128, 512, 768]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ctx_length in ctx_lengths:\n",
    "    max_ctx_length = ctx_length\n",
    "\n",
    "    # Load the model and tokenizer\n",
    "    model, tokenizer = load_qwen()\n",
    "\n",
    "    # Process the data into sequences of text\n",
    "    train_texts, val_texts, test_texts = load_and_preprocess(\"lotka_volterra_data.h5\", test_size=test_size)\n",
    "\n",
    "\n",
    "    # Defines the maximum context length for the model\n",
    "    train_input_ids = process_sequences(\n",
    "        train_texts, tokenizer, max_ctx_length, stride=max_ctx_length // 2\n",
    "    )\n",
    "    val_input_ids = process_sequences(\n",
    "        val_texts, tokenizer, max_ctx_length, stride=max_ctx_length\n",
    "    )\n",
    "    test_texts_all, test_input_ids_some = process_data(\n",
    "        test_texts, tokenizer, points=points\n",
    "    )\n",
    "\n",
    "    # Create data loaders\n",
    "    train_dataset = TensorDataset(train_input_ids)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    val_dataset = TensorDataset(val_input_ids)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    test_dataset = TensorDataset(test_input_ids_some)\n",
    "    test_loader = DataLoader(test_dataset, shuffle=False)\n",
    "\n",
    "    # Dictionary to store results\n",
    "    grid_results = {}\n",
    "\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Training with max_ctx_length={max_ctx_length}\")\n",
    "    print(f\"{'='*50}\\n\")\n",
    "\n",
    "    # Actually apply LoRA to the model:\n",
    "    for layer in model.model.layers:\n",
    "        layer.self_attn.q_proj = LoRALinear(layer.self_attn.q_proj, r=lora_rank, alpha = lora_alpha)\n",
    "        layer.self_attn.v_proj = LoRALinear(layer.self_attn.v_proj, r=lora_rank, alpha = lora_alpha)\n",
    "    # ^These are the parts that will actually be trained!\n",
    "\n",
    "    # Freeze all layers except the LoRA layers\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"A\" in name or \"B\" in name:\n",
    "            param.requires_grad = True\n",
    "        else:\n",
    "            param.requires_grad = False\n",
    "\n",
    "    # Create optimizer with current learning rate\n",
    "    optimizer = torch.optim.Adam(\n",
    "        (p for p in model.parameters() if p.requires_grad), \n",
    "        lr=learning_rate, \n",
    "    )\n",
    "\n",
    "    # Prepare with accelerator\n",
    "    # Use mixed precision training as it is faster and consumes less memory\n",
    "    accelerator = Accelerator(mixed_precision='fp16')\n",
    "    model, optimizer, train_loader_local, val_loader_local, test_loader_local = accelerator.prepare(\n",
    "        model, optimizer, train_loader, val_loader, test_loader\n",
    "    )\n",
    "\n",
    "    # Train the model (shortened training for grid search)\n",
    "    steps = 0\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    early_stop_steps = min(max_steps, 500)  # Reduce training for grid search\n",
    "\n",
    "    while steps < early_stop_steps:\n",
    "        progress_bar = tqdm(train_loader_local, desc=f\"Steps {steps}\")\n",
    "        for (batch,) in progress_bar:\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch, labels=batch)\n",
    "            loss = outputs.loss\n",
    "            train_losses.append([loss.item(), steps])\n",
    "            accelerator.backward(loss)\n",
    "            optimizer.step()\n",
    "            \n",
    "            if (steps % 50) == 0:\n",
    "                avg_loss = evaluate_model(model, val_loader_local, steps)\n",
    "                val_losses.append([avg_loss, steps])\n",
    "                model.train()\n",
    "                \n",
    "            steps += 1\n",
    "            progress_bar.set_postfix(loss=loss.item())\n",
    "            \n",
    "            if steps >= early_stop_steps:\n",
    "                break\n",
    "\n",
    "    # Final evaluation\n",
    "    final_val_loss = evaluate_model(model, val_loader_local, steps)\n",
    "\n",
    "    # Calculate the FLOPs\n",
    "    model_flops = Flopper(sequence_length = max_ctx_length, model=model, num_steps_training=max_steps, batch_size = batch_size, use_lora=True, lora_rank=lora_rank)\n",
    "    flops = model_flops.compute_flops()\n",
    "    print(f\"FLOPs: {model_flops}\")\n",
    "\n",
    "    # Store results\n",
    "    grid_results[(max_ctx_length)] = {\n",
    "        \"final_val_loss\": final_val_loss,\n",
    "        \"train_losses\": train_losses,\n",
    "        \"val_losses\": val_losses,\n",
    "        \"flops\": flops\n",
    "    }\n",
    "\n",
    "    # Save results\n",
    "    joblib.dump(grid_results, f\"../results/grid_results_{max_ctx_length}.joblib\")\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():    \n",
    "        for (batch,) in tqdm(test_loader_local):\n",
    "            outputs = model.generate(batch, attention_mask = torch.ones_like(batch), max_new_tokens=max_ctx_length*2)\n",
    "            prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            prediction_decoded = decoding(prediction)\n",
    "            break\n",
    "            \n",
    "\n",
    "    test_decoded = decoding(test_texts_all[0])\n",
    "\n",
    "    colours = ['#7b3294', '#c2a5cf', '#a6dba0', '#008837']\n",
    "    _, _, time = get_dataset()\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    ax.plot(time, prediction_decoded[0], label='Prey (predicted)', linestyle=\"--\", zorder=100, color=colours[0], linewidth=2)\n",
    "    ax.plot(time, prediction_decoded[1], label='Predator (predicted)', linestyle=\"--\", zorder=100, color=colours[3], linewidth=2)\n",
    "    ax.plot(time, test_decoded[0], label='Prey (true)', color=colours[1], linewidth=2)\n",
    "    ax.plot(time, test_decoded[1], label='Predator (true)', color=colours[2], linewidth=2)\n",
    "    ax.axvline(time[79], color='black', linestyle='--', label='Prediction start')\n",
    "    ax.set_xlabel('Time')\n",
    "    ax.set_ylabel('Population')\n",
    "    ax.legend(fontsize=15, ncol=5, bbox_to_anchor=(0.5, 1.1), loc='center')\n",
    "    plt.savefig(f'../plots/{max_ctx_length}_inspection.png')\n",
    "    plt.show()\n",
    "\n",
    "    # Clean up\n",
    "    del model\n",
    "    del tokenizer\n",
    "    del optimizer\n",
    "    del train_loader_local\n",
    "    del val_loader_local\n",
    "    del test_loader_local\n",
    "    del accelerator\n",
    "    del train_losses\n",
    "    del val_losses\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_loss = 1000\n",
    "best_params = None\n",
    "\n",
    "all_train_loss = []\n",
    "all_val_loss = []\n",
    "all_keys = []\n",
    "\n",
    "best_val_loss = 1000\n",
    "best_file = None\n",
    "\n",
    "# Pattern to match files of the form grid_results_NUMBER.joblib\n",
    "pattern = r'grid_results_\\d+\\.joblib'\n",
    "\n",
    "# Get all files in the results directory\n",
    "result_files = glob.glob('../results/*')\n",
    "\n",
    "# Filter files that match the pattern (just one number after grid_results_)\n",
    "result_files = [file for file in result_files if re.search(pattern, file)]\n",
    "\n",
    "print(result_files)\n",
    "\n",
    "# Display the content of each file\n",
    "for file_path in result_files:\n",
    "    # Load the joblib file\n",
    "    result = joblib.load(file_path)\n",
    "    \n",
    "    key = next(iter(result.keys()))\n",
    "    print(f\"Key: {key}: \", result[key]['final_val_loss'])\n",
    "\n",
    "    all_train_loss.append(result[key]['train_losses'])\n",
    "    all_val_loss.append(result[key]['val_losses'])\n",
    "    all_keys.append(key)\n",
    "    \n",
    "    # Check if the current file has the best validation loss\n",
    "    if result[key]['final_val_loss'] < best_val_loss:\n",
    "        best_val_loss = result[key]['final_val_loss']\n",
    "        best_file = key\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"Best validation loss: {best_val_loss}\")\n",
    "print(f\"Best file: {best_file}\")\n",
    "print(f\"{'='*50}\")\n",
    "\n",
    "best_overall_params['max_ctx_length'] = best_file\n",
    "\n",
    "all_train_loss = np.array(all_train_loss)\n",
    "all_val_loss = np.array(all_val_loss)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "for i in range(len(all_train_loss)):\n",
    "    ax.plot(all_train_loss[i, :, 1], all_train_loss[i, :, 0], label=f\"{all_keys[i]}\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "for i in range(len(all_val_loss)):\n",
    "    ax.plot(all_val_loss[i, :, 1], all_val_loss[i, :, 0], label=f\"{all_keys[i]}\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_overall_params_load = joblib.load(\"best_overall_params.joblib\")\n",
    "\n",
    "lora_rank = best_overall_params_load['lora_rank']\n",
    "lora_alpha = 2*lora_rank\n",
    "batch_size = 4\n",
    "learning_rate = best_overall_params_load['learning_rate']\n",
    "test_size = 0.2\n",
    "max_steps = 200\n",
    "max_ctx_length = best_overall_params_load['max_ctx_length']\n",
    "points = 80\n",
    "\n",
    "schedulers = ['StepLR', 'CosineAnnealingLR', 'CosineScheduleWithWarmup']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for schedule_choice in schedulers:\n",
    "    if schedule_choice == 'StepLR':\n",
    "        step_size = 100\n",
    "        gamma = 0.1\n",
    "    elif schedule_choice == 'CosineAnnealingLR':\n",
    "        T_max = max_steps\n",
    "    elif schedule_choice == 'CosineScheduleWithWarmup':\n",
    "        warmup_steps = int(0.075*max_steps)\n",
    "        num_training_steps = max_steps\n",
    "\n",
    "    # Load the model and tokenizer\n",
    "    model, tokenizer = load_qwen()\n",
    "\n",
    "    # Process the data into sequences of text\n",
    "    train_texts, val_texts, test_texts = load_and_preprocess(\"lotka_volterra_data.h5\", test_size=test_size)\n",
    "\n",
    "\n",
    "    # Defines the maximum context length for the model\n",
    "    train_input_ids = process_sequences(\n",
    "        train_texts, tokenizer, max_ctx_length, stride=max_ctx_length // 2\n",
    "    )\n",
    "    val_input_ids = process_sequences(\n",
    "        val_texts, tokenizer, max_ctx_length, stride=max_ctx_length\n",
    "    )\n",
    "    test_texts_all, test_input_ids_some = process_data(\n",
    "        test_texts, tokenizer, points=points\n",
    "    )\n",
    "\n",
    "    # Create data loaders\n",
    "\n",
    "    train_dataset = TensorDataset(train_input_ids)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    val_dataset = TensorDataset(val_input_ids)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    test_dataset = TensorDataset(test_input_ids_some)\n",
    "    test_loader = DataLoader(test_dataset, shuffle=False)\n",
    "\n",
    "    # Dictionary to store results\n",
    "    grid_results = {}\n",
    "\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Training with scheduler={schedule_choice}\")\n",
    "    print(f\"{'='*50}\\n\")\n",
    "\n",
    "    # Actually apply LoRA to the model:\n",
    "    for layer in model.model.layers:\n",
    "        layer.self_attn.q_proj = LoRALinear(layer.self_attn.q_proj, r=lora_rank, alpha = lora_alpha)\n",
    "        layer.self_attn.v_proj = LoRALinear(layer.self_attn.v_proj, r=lora_rank, alpha = lora_alpha)\n",
    "    # ^These are the parts that will actually be trained!\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"A\" in name or \"B\" in name:\n",
    "            param.requires_grad = True\n",
    "        else:\n",
    "            param.requires_grad = False\n",
    "\n",
    "    # Freeze all layers except the LoRA layers\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"A\" in name or \"B\" in name:\n",
    "            param.requires_grad = True\n",
    "        else:\n",
    "            param.requires_grad = False\n",
    "\n",
    "    # Create optimizer with current learning rate\n",
    "    optimizer = torch.optim.Adam(\n",
    "        (p for p in model.parameters() if p.requires_grad), \n",
    "        lr=learning_rate, \n",
    "    )\n",
    "\n",
    "    # Create scheduler\n",
    "    if schedule_choice == 'StepLR':\n",
    "        scheduler = StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "    elif schedule_choice == 'CosineAnnealingLR':\n",
    "        scheduler = CosineAnnealingLR(optimizer, T_max=T_max)\n",
    "    elif schedule_choice == 'CosineScheduleWithWarmup':\n",
    "        scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=num_training_steps)\n",
    "\n",
    "    #Prepare with accelerator\n",
    "    accelerator = Accelerator(mixed_precision='fp16')\n",
    "    model, optimizer, scheduler, train_loader_local, val_loader_local, test_loader_local = accelerator.prepare(\n",
    "        model, optimizer, scheduler, train_loader, val_loader, test_loader\n",
    "    )\n",
    "\n",
    "    # Train the model (shortened training for grid search)\n",
    "    steps = 0\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    early_stop_steps = min(max_steps, 500)  # Reduce training for grid search\n",
    "\n",
    "    while steps < early_stop_steps:\n",
    "        progress_bar = tqdm(train_loader_local, desc=f\"Steps {steps}\")\n",
    "        for (batch,) in progress_bar:\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch, labels=batch)\n",
    "            loss = outputs.loss\n",
    "            train_losses.append([loss.item(), steps])\n",
    "            accelerator.backward(loss)\n",
    "            optimizer.step()\n",
    "            \n",
    "            if steps % 50 == 0:\n",
    "                avg_loss = evaluate_model(model, val_loader_local, steps)\n",
    "                val_losses.append([avg_loss, steps])\n",
    "                model.train()\n",
    "                \n",
    "            steps += 1\n",
    "            progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "            del loss\n",
    "            del outputs\n",
    "            del batch\n",
    "            \n",
    "            if steps >= early_stop_steps:\n",
    "                break\n",
    "\n",
    "    # Final evaluation\n",
    "    final_val_loss = evaluate_model(model, val_loader_local, steps)\n",
    "\n",
    "    # Store results\n",
    "    grid_results[(schedule_choice)] = {\n",
    "        \"final_val_loss\": final_val_loss,\n",
    "        \"train_losses\": train_losses,\n",
    "        \"val_losses\": val_losses,\n",
    "    }\n",
    "\n",
    "    if schedule_choice == 'StepLR':\n",
    "        grid_results[(schedule_choice)][\"step_size\"] = step_size\n",
    "        grid_results[(schedule_choice)][\"gamma\"] = gamma\n",
    "    elif schedule_choice == 'CosineAnnealingLR':\n",
    "        grid_results[(schedule_choice)][\"T_max\"] = T_max\n",
    "    elif schedule_choice == 'CosineScheduleWithWarmup':\n",
    "        grid_results[(schedule_choice)][\"warmup_steps\"] = warmup_steps\n",
    "        grid_results[(schedule_choice)][\"num_training_steps\"] = num_training_steps\n",
    "\n",
    "    # Calculate the FLOPs\n",
    "    model_flops = Flopper(sequence_length = max_ctx_length, model=model, num_steps_training=max_steps, batch_size = batch_size, use_lora=True, lora_rank=lora_rank)\n",
    "    flops = model_flops.compute_flops()\n",
    "    print(f\"FLOPs: {flops}\")\n",
    "\n",
    "    grid_results[(schedule_choice)]={\n",
    "        \"flops\": flops\n",
    "    }\n",
    "\n",
    "    joblib.dump(grid_results, f\"../results/grid_results_{schedule_choice}.joblib\")\n",
    "\n",
    "    # Test the model\n",
    "    model.eval()\n",
    "    with torch.no_grad():    \n",
    "        for (batch,) in tqdm(test_loader_local):\n",
    "            outputs = model.generate(batch, attention_mask = torch.ones_like(batch), max_new_tokens=max_ctx_length*2)\n",
    "            prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            prediction_decoded = decoding(prediction)\n",
    "            break\n",
    "\n",
    "    test_decoded = decoding(test_texts_all[0])\n",
    "\n",
    "    colours = ['#7b3294', '#c2a5cf', '#a6dba0', '#008837']\n",
    "    _, _, time = get_dataset()\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    ax.plot(time, prediction_decoded[0], label='Prey (predicted)', linestyle=\"--\", zorder=100, color=colours[0], linewidth=2)\n",
    "    ax.plot(time, prediction_decoded[1], label='Predator (predicted)', linestyle=\"--\", zorder=100, color=colours[3], linewidth=2)\n",
    "    ax.plot(time, test_decoded[0], label='Prey (true)', color=colours[1], linewidth=2)\n",
    "    ax.plot(time, test_decoded[1], label='Predator (true)', color=colours[2], linewidth=2)\n",
    "    ax.axvline(time[79], color='black', linestyle='--', label='Prediction start')\n",
    "    ax.set_xlabel('Time')\n",
    "    ax.set_ylabel('Population')\n",
    "    ax.legend(fontsize=15, ncol=5, bbox_to_anchor=(0.5, 1.1), loc='center')\n",
    "    plt.savefig(f'../plots/{schedule_choice}_inspection.png')\n",
    "    plt.show()\n",
    "\n",
    "    # Clean up\n",
    "    del model\n",
    "    del tokenizer\n",
    "    del optimizer\n",
    "    del train_loader_local\n",
    "    del val_loader_local\n",
    "    del test_loader_local\n",
    "    del accelerator\n",
    "    del train_losses\n",
    "    del val_losses\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_loss = 1000\n",
    "best_params = None\n",
    "\n",
    "all_train_loss = []\n",
    "all_val_loss = []\n",
    "all_keys = []\n",
    "\n",
    "best_val_loss = 1000\n",
    "best_file = None\n",
    "\n",
    "# Pattern to match files of the form grid_results_WORDS.joblib\n",
    "pattern = r'grid_results_[a-zA-Z]+\\.joblib'\n",
    "\n",
    "# Get all files in the results directory\n",
    "result_files = glob.glob('../results/*')\n",
    "\n",
    "# Filter files that match the pattern (just one number after grid_results_)\n",
    "result_files = [file for file in result_files if re.search(pattern, file)]\n",
    "\n",
    "print(result_files)\n",
    "\n",
    "# Display the content of each file\n",
    "for file_path in result_files:\n",
    "    # Load the joblib file\n",
    "    result = joblib.load(file_path)\n",
    "    \n",
    "    key = next(iter(result.keys()))\n",
    "    print(f\"Key: {key}: \", result[key]['final_val_loss'])\n",
    "\n",
    "    all_train_loss.append(result[key]['train_losses'])\n",
    "    all_val_loss.append(result[key]['val_losses'])\n",
    "    all_keys.append(key)\n",
    "    \n",
    "    # Check if the current file has the best validation loss\n",
    "    if result[key]['final_val_loss'] < best_val_loss:\n",
    "        best_val_loss = result[key]['final_val_loss']\n",
    "        best_file = key\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"Best validation loss: {best_val_loss}\")\n",
    "print(f\"Best file: {best_file}\")\n",
    "print(f\"{'='*50}\")\n",
    "\n",
    "best_overall_params['scheduler_choice'] = best_file\n",
    "\n",
    "all_train_loss = np.array(all_train_loss)\n",
    "all_val_loss = np.array(all_val_loss)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "for i in range(len(all_train_loss)):\n",
    "    ax.plot(all_train_loss[i, :, 1], all_train_loss[i, :, 0], label=f\"{all_keys[i]}\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "for i in range(len(all_val_loss)):\n",
    "    ax.plot(all_val_loss[i, :, 1], all_val_loss[i, :, 0], label=f\"{all_keys[i]}\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(best_overall_params, \"best_overall_params.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lora",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
