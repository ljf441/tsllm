%--------------------
% Packages
% -------------------
\documentclass[11pt,a4paper]{article}
\usepackage[utf8x]{inputenc}
\usepackage[T1]{fontenc}
%\usepackage{gentium}
\usepackage{mathptmx} % Use Times Font
% \usepackage{wordcount}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{minted}
\usepackage{pdflscape}
\usepackage[pdftex]{graphicx} % Required for including pictures
\usepackage[pdftex,linkcolor=black,pdfborder={0 0 0}]{hyperref} % Format links for pdf
\usepackage{calc} % To reset the counter in the document after title page
\usepackage[numbers]{natbib}
\usepackage{amssymb} % Required for \mathbb
\usepackage{amsmath} % Required for bmatrix environment
\frenchspacing % No double spacing between sentences
\linespread{1.2} % Set linespace
\usepackage[a4paper, lmargin=0.1666\paperwidth, rmargin=0.1666\paperwidth, tmargin=0.1111\paperheight, bmargin=0.1111\paperheight]{geometry} %margins
%\usepackage{parskip}

\usepackage[all]{nowidow} % Tries to remove widows
\usepackage[protrusion=true,expansion=true]{microtype} % Improves typography, load after fontpackage is selected
\newcommand{\apjs}{ApJS}
\newcommand{\apj}{ApJ}
\newcommand{\apjl}{ApJ}
\newcommand{\mnras}{MNRAS}
\newcommand{\aap}{A\&A}
\newcommand{\aj}{AJ}
\newcommand{\nat}{Nature}
\newcommand{\bain}{Bull.~Astron.~Inst.~Netherlands} 
\newcommand{\araa}{ARA\&A}
\newcommand{\icarus}{Icarus}
\setlength{\tabcolsep}{5pt} 
\renewcommand{\arraystretch}{0.8}

%-----------------------
% Set pdf information and add title, fill in the fields
%-----------------------
\hypersetup{ 	
pdfsubject = {Deep Learning Coursework},
pdftitle = {Deep Learning Coursework},
pdfauthor = {Laura Just Fung (lj441)}
}

\usepackage{hyperref}
\usepackage{cleveref}

%-----------------------
% Begin document
%-----------------------
\begin{document} 

\begin{center}
    \LARGE{\textbf{Deep Learning Coursework Assignment}}
    \\
    \Large{{Low-Rank Adaptation of Large Language Models for Time Series Forecasting}}
    \\
    \large{Laura Just Fung (lj441)}
    \\
    April 7, 2025
    \\
    Word count: 2196
\end{center}

\section{Introduction}
Time series forecasting is a challenging problem in machine learning when compared to other tasks such as image classification or natural language processing. This is because time series datasets are often comprised of incomplete data from different sources. Additionally, such problems often involve extrapolation from very small amounts of information \citep{gruver2024largelanguagemodelszeroshot}.

This report demonstrates that by encoding time series data as a string of digits, time series forecasting can be directly translated to next-token prediction in text. Thus, larga language models (LLMs) can be repurposed to this task fairly naturally, as shown by \citeauthor{gruver2024largelanguagemodelszeroshot}, who showed that LLMs are rather capable zero-shot time series forecasters. In this report, the Qwen2.5-0.5B-Instruct model \citep{yang2024qwen2technicalreport} along with Low-Rank Adaptation (LoRA) of the $q$ and $v$ projection matrices are used to explore this concept further and demonstrate performance of a LLM that has been fine-tuned towards this task.

\section{Compute constraints}
\label{sec:constraints}
A budget of $10^{17}$ floating point operations (FLOPS) was applied to this coursework, including all reported experiments as well as the final training run. The FLOPS accounting for primitives are defined in Table~\ref{tab:flops_primitives}. The extrapolated FLOPS for other operations are shown in Table~\ref{tab:flops_advanced}.

\begin{table}[h]
    \centering
    \begin{tabular}{c|c}
        Operations & FLOPS \\
        \hline
        Addition/Subtraction/Negation & 1 \\
        Multiplication/Division/Inverse & 1 \\
        ReLU/Absolute value & 1 \\
        Exponentiation/Logarithm & 10\\
        Sine/Cosine/Square root & 10 \\
        
    \end{tabular}
    \caption{Standardised FLOPS for common primitive operations as defined for the rest of this report.}
    \label{tab:flops_primitives}
\end{table}

\begin{table}
    \centering
    \begin{tabular}{c|c}
        Operation & FLOPS \\
        \hline
        $M_{m \times n} \times M_{n \times p}$ matrix multiplication & $mp(2n-1)$\\
        SiLU($M_{m \times n}$)& $13mn$ \\
        RMSNorm($M_{m \times n}$) & $m(5n + 10)$ \\
        Softmax($M_{m \times m}$) & $m(12m-1)$\\
        Embedding RoPE($M_{m \times n}$) & $mn$ \\ 
        Rank $r$ LoRA($M_{m \times m}$) & $2rm^2$
    \end{tabular}
    \caption{Extrapolated FLOPS for operations done whilst training and validating the LLM with LoRA.}
    \label{tab:flops_advanced}
\end{table}
While Qwen2.5-0.5B-Instruct uses SwiGLU activations as well as Grouped Query Attention, the FLOPS for SiLU activations as well as standard multi-headed attention have been calcualted for simplification. For similar reasons, it has been assumed that the FLOPS of backpropagation are exactly twice that of the forward pass. Additionally, for all reported FLOPS, any operations performed outside of the model are not included. Finally, the cost of computing the rotary positional encodings have been ignored.

\section{Preprocessing}
\label{zero}
A text-based numeric encoding method adapted from the time series data preprocesing scheme described by \citeauthor{gruver2024largelanguagemodelszeroshot} was implemented for all data given to Qwen2.5-0.5B-Instruct. This preprocessing ensures that the numeric sequences are appropriately formatted for the best performance of Qwen2.5-0.5B-Instruct's tokenizer and forecasting.

First, as the numeric values in the dataset may vary, to standardise the numeric range of the data and control the token length, a simple scaling was applied, as shown in Eq.~\ref{eq:scale}

\begin{equation}
    x'_t = \frac{x_t}{\alpha},
\end{equation}
where $\alpha$ is chosen based on the distribution of the dataset.

Additionally, the scaled numeric values were rounded to a fixed number of decimal places to ensure uniformity and consistent representation when tokenized.

The data used for this coursework involves multivariate Lotka-Volterra time series data. Thus, the following encoding is used, as shown in Eq.~\ref{eq:encode}:

\begin{equation}
    \begin{pmatrix}
        P_0 & Q_0 \\
        P_1 & Q_1 \\
        \vdots & \vdots \\
        P_t & Q_t 
        \end{pmatrix}  
        = P_0,Q_0;P_{1},Q_{1};\ldots;P_{t},Q_{t},
    \label{eq:encode}
\end{equation}
where $P_t$ refers to the prey value at time $t$ and $Q_t$ refers to the predator value at time $t$. The comma is used to separate the predator and prey values, while the semicolon is used to separate the time steps.

The code for this preprocessing is shown in Listing~\ref{lst:preprocess},
\begin{listing}[h]
\inputminted[firstline=64, lastline=83]{python}{../src/preprocessor.py}
\caption{Function to preprocess the time series data. The function takes in the prey and predator values, the scaling factor $\alpha$, and the number of decimal places to round to. It returns the encoded string representation of the time series data.}
\label{lst:preprocess}
\end{listing}
and this function can be found in the \texttt{preprocessor.py} file in the \texttt{src} directory.

As an example for the results for this preprocessing implementation with $\alpha = 5$ and 3 decimal places, the time series is encoded as shown in Eq.~\ref{eq:example}, with trailing zeros dropped:

\begin{equation}
\raggedleft
\begin{split}
\begin{pmatrix}
1.1335121  & 1.1031258 \\
0.55542254  & 1.2579137
\end{pmatrix}
= 2.267,2.206;1.111,2.516
\\ 
= [17, 13, 17, 21, 22, 11, 17, 13, 17, 15, 21, 26, 16, 13, 16, 16, 16, 11, 17, 13, 20, 16]
\end{split}
\label{eq:example}
\end{equation}



\bibliographystyle{vancouver-authoryear}
\bibliography{bibliography}
\appendix
\section{Use of auto-generation tools}
Auto-generation tools were used to help parse error messages throughout the project, and to help format this \LaTeX\ report.

Auto-generation tools were not used elsewhere, for code generation, writing, or otherwise.
\end{document}
