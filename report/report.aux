\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{gruver2024largelanguagemodelszeroshot}
\citation{gruver2024largelanguagemodelszeroshot}
\citation{yang2024qwen2technicalreport}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Compute constraints}{1}{section.2}\protected@file@percent }
\newlabel{sec:constraints}{{2}{1}{Compute constraints}{section.2}{}}
\newlabel{sec:constraints@cref}{{[section][2][]2}{[1][1][]1}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Standardised FLOPS for common primitive operations as defined for the rest of this report.}}{1}{table.1}\protected@file@percent }
\newlabel{tab:flops_primitives}{{1}{1}{Standardised FLOPS for common primitive operations as defined for the rest of this report}{table.1}{}}
\newlabel{tab:flops_primitives@cref}{{[table][1][]1}{[1][1][]1}}
\citation{gruver2024largelanguagemodelszeroshot}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Extrapolated FLOPS for operations done whilst training and validating the LLM with LoRA.}}{2}{table.2}\protected@file@percent }
\newlabel{tab:flops_advanced}{{2}{2}{Extrapolated FLOPS for operations done whilst training and validating the LLM with LoRA}{table.2}{}}
\newlabel{tab:flops_advanced@cref}{{[table][2][]2}{[1][1][]2}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Preprocessing}{2}{section.3}\protected@file@percent }
\newlabel{zero}{{3}{2}{Preprocessing}{section.3}{}}
\newlabel{zero@cref}{{[section][3][]3}{[1][2][]2}}
\newlabel{eq:scale}{{1}{2}{Preprocessing}{equation.3.1}{}}
\newlabel{eq:scale@cref}{{[equation][1][]1}{[1][2][]2}}
\@writefile{lol}{\contentsline {listing}{\numberline {1}{\ignorespaces Function to preprocess the time series data. The function takes in the prey and predator values, the scaling factor $\alpha $, and the number of decimal places to round to. It returns the encoded string representation of the time series data.}}{3}{listing.1}\protected@file@percent }
\newlabel{lst:preprocess}{{1}{3}{Preprocessing}{listing.1}{}}
\newlabel{lst:preprocess@cref}{{[listing][1][]1}{[1][3][]3}}
\newlabel{eq:encode}{{2}{3}{Preprocessing}{equation.3.2}{}}
\newlabel{eq:encode@cref}{{[equation][2][]2}{[1][2][]3}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Example of the preprocessing steps for two steps in the time series data. The raw data is scaled, encoded, and then tokenized.}}{4}{table.3}\protected@file@percent }
\newlabel{tab:example}{{3}{4}{Example of the preprocessing steps for two steps in the time series data. The raw data is scaled, encoded, and then tokenized}{table.3}{}}
\newlabel{tab:example@cref}{{[table][3][]3}{[1][3][]4}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Another example of the preprocessing steps for two steps in the time series data. The raw data is scaled, encoded, and then tokenized.}}{4}{table.4}\protected@file@percent }
\newlabel{tab:example2}{{4}{4}{Another example of the preprocessing steps for two steps in the time series data. The raw data is scaled, encoded, and then tokenized}{table.4}{}}
\newlabel{tab:example2@cref}{{[table][4][]4}{[1][3][]4}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Baseline}{5}{section.4}\protected@file@percent }
\newlabel{sec:baseline}{{4}{5}{Baseline}{section.4}{}}
\newlabel{sec:baseline@cref}{{[section][4][]4}{[1][5][]5}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Predictions of the untrained Qwen2.5-0.5B-Instruct model on system 870 of the Lotka-Volterra time series data. The model was given the first 80 points in the time-series data and asked to predict the next 20 points for both the predator (purple) and prey (gold) time series. The model's predictions are shown in the solid lines, while the true values are shown in the dashed lines.}}{5}{figure.1}\protected@file@percent }
\newlabel{fig:baseline_pred}{{1}{5}{Predictions of the untrained Qwen2.5-0.5B-Instruct model on system 870 of the Lotka-Volterra time series data. The model was given the first 80 points in the time-series data and asked to predict the next 20 points for both the predator (purple) and prey (gold) time series. The model's predictions are shown in the solid lines, while the true values are shown in the dashed lines}{figure.1}{}}
\newlabel{fig:baseline_pred@cref}{{[figure][1][]1}{[1][5][]5}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Metrics for the untrained Qwen2.5-0.5B-Instruct model performance on system 870 of the Lotka-Volterra time series data. The model was given the first 80 points in the time-series data and asked to predict the next 20 points. The metrics are calculated for both the overall time series and the out-of-distribution data only.}}{6}{table.5}\protected@file@percent }
\newlabel{tab:baseline}{{5}{6}{Metrics for the untrained Qwen2.5-0.5B-Instruct model performance on system 870 of the Lotka-Volterra time series data. The model was given the first 80 points in the time-series data and asked to predict the next 20 points. The metrics are calculated for both the overall time series and the out-of-distribution data only}{table.5}{}}
\newlabel{tab:baseline@cref}{{[table][5][]5}{[1][5][]6}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Running mean squared error (RMSE) for the untrained Qwen2.5-0.5B-Instruct model on system 870 of the Lotka-Volterra time series data. The model was given the first 80 points in the time-series data and asked to predict the next 20 points for both the predator (purple) and prey (gold) time series.}}{6}{figure.2}\protected@file@percent }
\newlabel{fig:baseline_rmse}{{2}{6}{Running mean squared error (RMSE) for the untrained Qwen2.5-0.5B-Instruct model on system 870 of the Lotka-Volterra time series data. The model was given the first 80 points in the time-series data and asked to predict the next 20 points for both the predator (purple) and prey (gold) time series}{figure.2}{}}
\newlabel{fig:baseline_rmse@cref}{{[figure][2][]2}{[1][5][]6}}
\bibstyle{vancouver-authoryear}
\bibdata{bibliography}
\bibcite{gruver2024largelanguagemodelszeroshot}{{1}{2024}{{Gruver et~al.}}{{Nate Gruver and Marc Finzi and Shikai Qiu and Andrew Gordon Wilson}}}
\bibcite{yang2024qwen2technicalreport}{{2}{2024}{{Yang et~al.}}{{An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jianxin Yang and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Xuejing Liu and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhifang Guo and Zhihao Fan}}}
\gdef\minted@oldcachelist{,
  default.pygstyle,
  B9FC7E34FB4D64478DECE298A669FC077C1A109740FF5E49AE684E149A3B6943.pygtex}
\@writefile{toc}{\contentsline {section}{\numberline {A}Use of auto-generation tools}{7}{appendix.A}\protected@file@percent }
\gdef \@abspage@last{7}
