\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{gruver2024largelanguagemodelszeroshot}
\citation{yang2024qwen2technicalreport}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Compute constraints}{1}{section.2}\protected@file@percent }
\newlabel{sec:constraints}{{2}{1}{Compute constraints}{section.2}{}}
\newlabel{sec:constraints@cref}{{[section][2][]2}{[1][1][]1}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Standardised FLOPS for common primitive operations as defined for the rest of this report.}}{1}{table.1}\protected@file@percent }
\newlabel{tab:flops_primitives}{{1}{1}{Standardised FLOPS for common primitive operations as defined for the rest of this report}{table.1}{}}
\newlabel{tab:flops_primitives@cref}{{[table][1][]1}{[1][1][]1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}FLOPS calculations}{1}{subsection.2.1}\protected@file@percent }
\newlabel{sec:flops_calc}{{2.1}{1}{FLOPS calculations}{subsection.2.1}{}}
\newlabel{sec:flops_calc@cref}{{[subsection][1][2]2.1}{[1][1][]1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.1}RoPE}{2}{subsubsection.2.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.2}Key, query, and value projections}{2}{subsubsection.2.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.3}RMS Norm}{2}{subsubsection.2.1.3}\protected@file@percent }
\newlabel{eq:rmsnorm}{{1}{2}{RMS Norm}{equation.2.1}{}}
\newlabel{eq:rmsnorm@cref}{{[equation][1][]1}{[1][2][]2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.4}Softmax}{2}{subsubsection.2.1.4}\protected@file@percent }
\newlabel{eq:softmax}{{2}{2}{Softmax}{equation.2.2}{}}
\newlabel{eq:softmax@cref}{{[equation][2][]2}{[1][2][]2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.5}SwiGLU}{2}{subsubsection.2.1.5}\protected@file@percent }
\citation{gruver2024largelanguagemodelszeroshot}
\newlabel{eq:silu}{{3}{3}{SwiGLU}{equation.2.3}{}}
\newlabel{eq:silu@cref}{{[equation][3][]3}{[1][3][]3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.6}LoRA linear layer}{3}{subsubsection.2.1.6}\protected@file@percent }
\newlabel{eq:lora}{{4}{3}{LoRA linear layer}{equation.2.4}{}}
\newlabel{eq:lora@cref}{{[equation][4][]4}{[1][3][]3}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Preprocessing}{3}{section.3}\protected@file@percent }
\newlabel{zero}{{3}{3}{Preprocessing}{section.3}{}}
\newlabel{zero@cref}{{[section][3][]3}{[1][3][]3}}
\newlabel{eq:scale}{{5}{3}{Preprocessing}{equation.3.5}{}}
\newlabel{eq:scale@cref}{{[equation][5][]5}{[1][3][]3}}
\newlabel{eq:encode}{{6}{4}{Preprocessing}{equation.3.6}{}}
\newlabel{eq:encode@cref}{{[equation][6][]6}{[1][3][]4}}
\@writefile{lol}{\contentsline {listing}{\numberline {1}{\ignorespaces The function takes in the prey and predator values, the scaling factor $\alpha $, and the number of decimal places to round to. It returns the encoded string representation of the time series data.}}{4}{listing.1}\protected@file@percent }
\newlabel{lst:preprocess}{{1}{4}{Preprocessing}{listing.1}{}}
\newlabel{lst:preprocess@cref}{{[listing][1][]1}{[1][4][]4}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Example of the preprocessing method for two steps in the time series data. The raw data is scaled, encoded, and then tokenized.}}{5}{table.2}\protected@file@percent }
\newlabel{tab:example}{{2}{5}{Example of the preprocessing method for two steps in the time series data. The raw data is scaled, encoded, and then tokenized}{table.2}{}}
\newlabel{tab:example@cref}{{[table][2][]2}{[1][4][]5}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Another example of the preprocessing method for two steps in the time series data.}}{5}{table.3}\protected@file@percent }
\newlabel{tab:example2}{{3}{5}{Another example of the preprocessing method for two steps in the time series data}{table.3}{}}
\newlabel{tab:example2@cref}{{[table][3][]3}{[1][4][]5}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Baseline}{6}{section.4}\protected@file@percent }
\newlabel{sec:baseline}{{4}{6}{Baseline}{section.4}{}}
\newlabel{sec:baseline@cref}{{[section][4][]4}{[1][6][]6}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Metrics for the untrained model performance on a subset of the test set.}}{6}{table.4}\protected@file@percent }
\newlabel{tab:baseline}{{4}{6}{Metrics for the untrained model performance on a subset of the test set}{table.4}{}}
\newlabel{tab:baseline@cref}{{[table][4][]4}{[1][6][]6}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Predictions (dashed) of the untrained model compared with the true (solid) values on a subset of the dataset for both predator (purple) and prey (gold).}}{8}{figure.1}\protected@file@percent }
\newlabel{fig:baseline_pred}{{1}{8}{Predictions (dashed) of the untrained model compared with the true (solid) values on a subset of the dataset for both predator (purple) and prey (gold)}{figure.1}{}}
\newlabel{fig:baseline_pred@cref}{{[figure][1][]1}{[1][6][]8}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Running mean squared error (RMSE) for the model on a subset of the test set over the out-of-distribution data for the predator (purple) and prey (gold) time series.}}{9}{figure.2}\protected@file@percent }
\newlabel{fig:baseline_rmse}{{2}{9}{Running mean squared error (RMSE) for the model on a subset of the test set over the out-of-distribution data for the predator (purple) and prey (gold) time series}{figure.2}{}}
\newlabel{fig:baseline_rmse@cref}{{[figure][2][]2}{[1][6][]9}}
\citation{hu2021loralowrankadaptationlarge}
\@writefile{toc}{\contentsline {section}{\numberline {5}LoRA}{10}{section.5}\protected@file@percent }
\newlabel{sec:lora}{{5}{10}{LoRA}{section.5}{}}
\newlabel{sec:lora@cref}{{[section][5][]5}{[1][10][]10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Training with LoRA}{10}{subsection.5.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Default hyperparameters used for training the model with LoRA on the dataset.}}{10}{table.5}\protected@file@percent }
\newlabel{tab:lora_default}{{5}{10}{Default hyperparameters used for training the model with LoRA on the dataset}{table.5}{}}
\newlabel{tab:lora_default@cref}{{[table][5][]5}{[1][10][]10}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Training and validation loss for the model fine-tuned with LoRA on the dataset using default hyperparameters. The bottom figure compares the training (blue) and validation (red) loss using the same resolution of every 50 steps.}}{11}{figure.3}\protected@file@percent }
\newlabel{fig:lora_default_loss}{{3}{11}{Training and validation loss for the model fine-tuned with LoRA on the dataset using default hyperparameters. The bottom figure compares the training (blue) and validation (red) loss using the same resolution of every 50 steps}{figure.3}{}}
\newlabel{fig:lora_default_loss@cref}{{[figure][3][]3}{[1][10][]11}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Predictions (dashed) of the model fine-tuned with LoRA using default hyperparameters compared with the true (solid) values on a subset of the test set for both predator (purple) and prey (gold).}}{12}{figure.4}\protected@file@percent }
\newlabel{fig:lora_default_pred}{{4}{12}{Predictions (dashed) of the model fine-tuned with LoRA using default hyperparameters compared with the true (solid) values on a subset of the test set for both predator (purple) and prey (gold)}{figure.4}{}}
\newlabel{fig:lora_default_pred@cref}{{[figure][4][]4}{[1][10][]12}}
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces Metrics for the model fine-tuned with LoRA using default hyperparameters on a subset of the test set.}}{13}{table.6}\protected@file@percent }
\newlabel{tab:lora_default_metrics}{{6}{13}{Metrics for the model fine-tuned with LoRA using default hyperparameters on a subset of the test set}{table.6}{}}
\newlabel{tab:lora_default_metrics@cref}{{[table][6][]6}{[1][13][]13}}
\@writefile{lot}{\contentsline {table}{\numberline {7}{\ignorespaces Hyperparameters tuned for the model fine-tuned with LoRA on the dataset.}}{13}{table.7}\protected@file@percent }
\newlabel{tab:lora_tuning}{{7}{13}{Hyperparameters tuned for the model fine-tuned with LoRA on the dataset}{table.7}{}}
\newlabel{tab:lora_tuning@cref}{{[table][7][]7}{[1][13][]13}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Hyperparameter tuning}{13}{subsection.5.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {8}{\ignorespaces Final validation loss for the model fine-tuned with LoRA on the dataset using different combinations of $\eta $ and $r$.}}{14}{table.8}\protected@file@percent }
\newlabel{tab:lora_lr_rank}{{8}{14}{Final validation loss for the model fine-tuned with LoRA on the dataset using different combinations of $\eta $ and $r$}{table.8}{}}
\newlabel{tab:lora_lr_rank@cref}{{[table][8][]8}{[1][14][]14}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.1}Learning rate and LoRA rank}{14}{subsubsection.5.2.1}\protected@file@percent }
\newlabel{sec:lora_lr}{{5.2.1}{14}{Learning rate and LoRA rank}{subsubsection.5.2.1}{}}
\newlabel{sec:lora_lr@cref}{{[subsubsection][1][5,2]5.2.1}{[1][13][]14}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Training (top) and validation (bottom) loss for the model fine-tuned with LoRA on the dataset using different $\eta $ and $r$.}}{15}{figure.5}\protected@file@percent }
\newlabel{fig:lora_lr_rank_loss}{{5}{15}{Training (top) and validation (bottom) loss for the model fine-tuned with LoRA on the dataset using different $\eta $ and $r$}{figure.5}{}}
\newlabel{fig:lora_lr_rank_loss@cref}{{[figure][5][]5}{[1][13][]15}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.2}Maximum context length}{16}{subsubsection.5.2.2}\protected@file@percent }
\newlabel{sec:max_ctx}{{5.2.2}{16}{Maximum context length}{subsubsection.5.2.2}{}}
\newlabel{sec:max_ctx@cref}{{[subsubsection][2][5,2]5.2.2}{[1][16][]16}}
\@writefile{lot}{\contentsline {table}{\numberline {9}{\ignorespaces Final validation loss for the model fine-tuned with LoRA on the dataset using different maximum context lengths with $\eta =0.0001$ and $r=8$.}}{16}{table.9}\protected@file@percent }
\newlabel{tab:lora_max_context}{{9}{16}{Final validation loss for the model fine-tuned with LoRA on the dataset using different maximum context lengths with $\eta =0.0001$ and $r=8$}{table.9}{}}
\newlabel{tab:lora_max_context@cref}{{[table][9][]9}{[1][16][]16}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Training (top) and validation (bottom) loss for the model fine-tuned with LoRA on the dataset using different maximum context lengths with $\eta =0.0001$ and $r=8$.}}{17}{figure.6}\protected@file@percent }
\newlabel{fig:lora_max_context_loss}{{6}{17}{Training (top) and validation (bottom) loss for the model fine-tuned with LoRA on the dataset using different maximum context lengths with $\eta =0.0001$ and $r=8$}{figure.6}{}}
\newlabel{fig:lora_max_context_loss@cref}{{[figure][6][]6}{[1][16][]17}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Final model}{18}{subsection.5.3}\protected@file@percent }
\newlabel{sec:final_model}{{5.3}{18}{Final model}{subsection.5.3}{}}
\newlabel{sec:final_model@cref}{{[subsection][3][5]5.3}{[1][18][]18}}
\@writefile{lot}{\contentsline {table}{\numberline {10}{\ignorespaces Final best hyperparameters for the model fine-tuned with LoRA on the dataset.}}{18}{table.10}\protected@file@percent }
\newlabel{tab:lora_final_hyperparameters}{{10}{18}{Final best hyperparameters for the model fine-tuned with LoRA on the dataset}{table.10}{}}
\newlabel{tab:lora_final_hyperparameters@cref}{{[table][10][]10}{[1][18][]18}}
\@writefile{lot}{\contentsline {table}{\numberline {11}{\ignorespaces Metrics for the model fine-tuned with LoRA using the final best hyperparameters on a subset from the test set.}}{18}{table.11}\protected@file@percent }
\newlabel{tab:lora_final_metrics}{{11}{18}{Metrics for the model fine-tuned with LoRA using the final best hyperparameters on a subset from the test set}{table.11}{}}
\newlabel{tab:lora_final_metrics@cref}{{[table][11][]11}{[1][18][]18}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Training (top) and validation (middle) loss for the model fine-tuned with LoRA on the dataset using the final best hyperparameters. The bottom figure directly compares the training (blue) and validation (red) loss using the same resolution of every 50 steps.}}{19}{figure.7}\protected@file@percent }
\newlabel{fig:lora_final_loss}{{7}{19}{Training (top) and validation (middle) loss for the model fine-tuned with LoRA on the dataset using the final best hyperparameters. The bottom figure directly compares the training (blue) and validation (red) loss using the same resolution of every 50 steps}{figure.7}{}}
\newlabel{fig:lora_final_loss@cref}{{[figure][7][]7}{[1][18][]19}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Predictions (dashed) of the model fine-tuned with LoRA using the final best hyperparameters compared with the true (solid) values on a subset of the test set for both predator (purple) and prey (gold).}}{20}{figure.8}\protected@file@percent }
\newlabel{fig:lora_final_pred}{{8}{20}{Predictions (dashed) of the model fine-tuned with LoRA using the final best hyperparameters compared with the true (solid) values on a subset of the test set for both predator (purple) and prey (gold)}{figure.8}{}}
\newlabel{fig:lora_final_pred@cref}{{[figure][8][]8}{[1][18][]20}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Running mean squared error (RMSE) for the model fine-tuned with LoRA using the final best hyperparameters on a subset of the test set over the out-of-distribution data for the predator (purple) and prey (gold) time series.}}{21}{figure.9}\protected@file@percent }
\newlabel{fig:lora_final_rmse}{{9}{21}{Running mean squared error (RMSE) for the model fine-tuned with LoRA using the final best hyperparameters on a subset of the test set over the out-of-distribution data for the predator (purple) and prey (gold) time series}{figure.9}{}}
\newlabel{fig:lora_final_rmse@cref}{{[figure][9][]9}{[1][18][]21}}
\@writefile{toc}{\contentsline {section}{\numberline {6}FLOPS usage}{22}{section.6}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {12}{\ignorespaces FLOPS usage for each stage of the coursework including inference by the untrained model; training and inference of the LoRA adapted model with default hyperparameters; grid search of $\eta $, LoRA rank, and context length; and the training and inference of the final model.}}{22}{table.12}\protected@file@percent }
\newlabel{tab:flops_track}{{12}{22}{FLOPS usage for each stage of the coursework including inference by the untrained model; training and inference of the LoRA adapted model with default hyperparameters; grid search of $\eta $, LoRA rank, and context length; and the training and inference of the final model}{table.12}{}}
\newlabel{tab:flops_track@cref}{{[table][12][]12}{[1][22][]22}}
\citation{hu2021loralowrankadaptationlarge}
\@writefile{toc}{\contentsline {section}{\numberline {7}Further improvements}{23}{section.7}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {8}Summary}{23}{section.8}\protected@file@percent }
\bibstyle{vancouver-authoryear}
\bibdata{bibliography}
\bibcite{gruver2024largelanguagemodelszeroshot}{{1}{2024}{{Gruver et~al.}}{{Nate Gruver and Marc Finzi and Shikai Qiu and Andrew Gordon Wilson}}}
\bibcite{hu2021loralowrankadaptationlarge}{{2}{2021}{{Hu et~al.}}{{Edward J. Hu and Yelong Shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Lu Wang and Weizhu Chen}}}
\bibcite{yang2024qwen2technicalreport}{{3}{2024}{{Yang et~al.}}{{An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jianxin Yang and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Xuejing Liu and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhifang Guo and Zhihao Fan}}}
\gdef\minted@oldcachelist{,
  default.pygstyle,
  6E1C69EE4C7ED986D98890AFB5517EBE7C1A109740FF5E49AE684E149A3B6943.pygtex}
\@writefile{toc}{\contentsline {section}{\numberline {A}Use of auto-generation tools}{24}{appendix.A}\protected@file@percent }
\gdef \@abspage@last{24}
